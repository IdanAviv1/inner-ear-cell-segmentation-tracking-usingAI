{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVnLk0JfrogF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "U-Net Training Script\n",
        "\n",
        "Requirements:\n",
        "    Python >= 3.9\n",
        "    pip install numpy opencv-python tifffile matplotlib pillow scipy pandas scikit-learn tensorflow\n",
        "\n",
        "Data folders:\n",
        "    IMAGE_DIR -> frame_00000_ch1.tif, frame_00000_ch2.tif, ...\n",
        "    MASK_DIR  -> frame_00000.tif  (labels encoded as 0/85/170/255)\n",
        "\n",
        "Conventions used:\n",
        "    - PEP 8 style: snake_case for functions/variables, UPPER_CASE for constants.\n",
        "    - Clear top-of-file CONFIG so users can easily change key parameters/paths.\n",
        "    - Short, precise English docstrings for every function/class/method:\n",
        "      Purpose, Inputs, Outputs/Returns, and behavior notes where relevant.\n",
        "\"\"\"\n",
        "\n",
        "# =========================\n",
        "# USER CONFIG (EDIT HERE)\n",
        "# =========================\n",
        "SAVE_DIR   = \"./output_train\"                   # Single folder for all outputs (weights, logs, optional full models)\n",
        "IMAGE_DIR  = \"./training_data/Full_DataSet/data\"\n",
        "MASK_DIR   = \"./training_data/Full_DataSet/label\"\n",
        "\n",
        "SIZE         = 256\n",
        "IN_CHANNELS  = 2\n",
        "NUM_CLASSES  = 4\n",
        "BATCH_SIZE   = 8\n",
        "EPOCHS       = 100\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "VAL_SPLIT     = 0.1\n",
        "RANDOM_STATE  = 0\n",
        "\n",
        "CLASS_WEIGHTS = [4, 2, 2, 1]\n",
        "\n",
        "# Resume control:\n",
        "RESUME_TRAINING = True        # True: load model_last.weights.h5 + continue from CSV; False: start fresh\n",
        "\n",
        "# Saving cadence:\n",
        "SAVE_WEIGHTS_EVERY_N = 1      # Save weights every N epochs (periodic snapshots)\n",
        "SAVE_FULL_MODEL_EVERY_N = None  # None to disable; or e.g. 10 to also save a full model every 10 epochs\n",
        "\n",
        "# =========================\n",
        "# DERIVED PATHS\n",
        "# =========================\n",
        "import os\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "WEIGHTS_PATH         = os.path.join(SAVE_DIR, \"model_last.weights.h5\")\n",
        "BEST_WEIGHTS_PATH    = os.path.join(SAVE_DIR, \"model_best.weights.h5\")\n",
        "LOG_PATH             = os.path.join(SAVE_DIR, \"training_log.csv\")\n",
        "FULL_MODEL_LAST_PATH = os.path.join(SAVE_DIR, \"model_last.h5\")\n",
        "FULL_MODEL_BEST_PATH = os.path.join(SAVE_DIR, \"model_best.h5\")\n",
        "\n",
        "# =========================\n",
        "# IMPORTS\n",
        "# =========================\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorflow.keras.utils import to_categorical, Sequence\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D, BatchNormalization, Activation, MaxPool2D,\n",
        "    Conv2DTranspose, Concatenate, Input\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, Callback\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# =========================\n",
        "# LOSS\n",
        "# =========================\n",
        "def weighted_categorical_crossentropy(weights):\n",
        "    \"\"\"\n",
        "    Weighted categorical crossentropy loss.\n",
        "\n",
        "    Purpose:\n",
        "        Apply class-dependent weights to the standard categorical crossentropy.\n",
        "\n",
        "    Inputs:\n",
        "        weights (list[float]): Class weights of length NUM_CLASSES.\n",
        "\n",
        "    Returns:\n",
        "        loss (callable): Keras-compatible loss function:\n",
        "            Args:\n",
        "                y_true: one-hot mask, shape (B, H, W, NUM_CLASSES)\n",
        "                y_pred: softmax probabilities, shape (B, H, W, NUM_CLASSES)\n",
        "            Returns:\n",
        "                Tensor of shape (B, H, W) with per-pixel loss values.\n",
        "    \"\"\"\n",
        "    weights = K.variable(weights)\n",
        "    def loss(y_true, y_pred):\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        loss_val = y_true * K.log(y_pred) * weights\n",
        "        loss_val = -K.sum(loss_val, -1)\n",
        "        return loss_val\n",
        "    return loss\n",
        "\n",
        "loss_fn = weighted_categorical_crossentropy(CLASS_WEIGHTS)\n",
        "\n",
        "# =========================\n",
        "# MODEL (U-Net)\n",
        "# =========================\n",
        "def conv_block(inputs, num_filters):\n",
        "    \"\"\"\n",
        "    Two Conv2D + BatchNorm + ReLU layers.\n",
        "\n",
        "    Inputs:\n",
        "        inputs: input tensor.\n",
        "        num_filters (int): number of convolution filters.\n",
        "\n",
        "    Returns:\n",
        "        Tensor after two conv-BN-ReLU operations with 'same' padding.\n",
        "    \"\"\"\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "def encoder_block(inputs, num_filters):\n",
        "    \"\"\"\n",
        "    Encoder stage: conv_block + 2x2 MaxPool.\n",
        "\n",
        "    Inputs:\n",
        "        inputs: input tensor.\n",
        "        num_filters (int): filters for conv_block.\n",
        "\n",
        "    Returns:\n",
        "        skip (Tensor): features before pooling (for decoder skip connection).\n",
        "        pooled (Tensor): downsampled features (MaxPool).\n",
        "    \"\"\"\n",
        "    x = conv_block(inputs, num_filters)\n",
        "    p = MaxPool2D((2,2))(x)\n",
        "    return x, p\n",
        "\n",
        "def decoder_block(inputs, skip, num_filters):\n",
        "    \"\"\"\n",
        "    Decoder stage: up-convolution + concatenate skip + conv_block.\n",
        "\n",
        "    Inputs:\n",
        "        inputs: decoder input tensor.\n",
        "        skip: corresponding encoder skip tensor.\n",
        "        num_filters (int): filters for conv_block.\n",
        "\n",
        "    Returns:\n",
        "        Tensor after upsampling, concatenation, and conv_block.\n",
        "    \"\"\"\n",
        "    x = Conv2DTranspose(num_filters, (2,2), strides=2, padding=\"same\")(inputs)\n",
        "    x = Concatenate()([x, skip])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "def build_unet(input_shape):\n",
        "    \"\"\"\n",
        "    Build the U-Net architecture used for multi-class segmentation.\n",
        "\n",
        "    Inputs:\n",
        "        input_shape (tuple): (H, W, C) e.g., (SIZE, SIZE, IN_CHANNELS)\n",
        "\n",
        "    Returns:\n",
        "        model (tf.keras.Model): U-Net with softmax output of NUM_CLASSES channels.\n",
        "    \"\"\"\n",
        "    inputs = Input(input_shape)\n",
        "    s1, p1 = encoder_block(inputs, 64)\n",
        "    s2, p2 = encoder_block(p1, 128)\n",
        "    s3, p3 = encoder_block(p2, 256)\n",
        "    s4, p4 = encoder_block(p3, 512)\n",
        "    b1 = conv_block(p4, 1024)\n",
        "    d1 = decoder_block(b1, s4, 512)\n",
        "    d2 = decoder_block(d1, s3, 256)\n",
        "    d3 = decoder_block(d2, s2, 128)\n",
        "    d4 = decoder_block(d3, s1, 64)\n",
        "    outputs = Conv2D(NUM_CLASSES, 1, padding=\"same\", activation=\"softmax\")(d4)\n",
        "    model = Model(inputs, outputs, name=\"UNET\")\n",
        "    return model\n",
        "\n",
        "# =========================\n",
        "# DATA GENERATOR\n",
        "# =========================\n",
        "class TiffDataGenerator(Sequence):\n",
        "    \"\"\"\n",
        "    Keras Sequence that streams paired channels and segmentation masks from disk.\n",
        "\n",
        "    Purpose:\n",
        "        - Load grayscale channels ch1/ch2 and a mask per sample.\n",
        "        - Apply CLAHE + Gaussian blur + percentile normalization (1â€“99).\n",
        "        - Map LUT mask values {0, 85, 170, 255} to class indices {0, 1, 2, 3}.\n",
        "        - Yield (images, masks) batches for training/validation.\n",
        "\n",
        "    Init Inputs:\n",
        "        image_dir (str): folder of ch1/ch2 TIFFs (e.g., frame_XXXXX_ch1.tif / _ch2.tif).\n",
        "        mask_dir  (str): folder of label TIFFs (frame_XXXXX.tif).\n",
        "        filenames (list[str]): base names (without suffixes) to load.\n",
        "        batch_size (int): batch size.\n",
        "        shuffle (bool): whether to shuffle after each epoch.\n",
        "\n",
        "    Yields:\n",
        "        images (np.ndarray): float32, shape (B, SIZE, SIZE, 2).\n",
        "        masks  (np.ndarray): one-hot uint8->float, shape (B, SIZE, SIZE, NUM_CLASSES).\n",
        "    \"\"\"\n",
        "    def __init__(self, image_dir, mask_dir, filenames, batch_size=8, shuffle=True):\n",
        "        super().__init__()\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.filenames = filenames\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.SIZE = SIZE\n",
        "        self.NUM_CLASSES = NUM_CLASSES\n",
        "        self.clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            int: number of batches per epoch (ceil(N / batch_size)).\n",
        "        \"\"\"\n",
        "        return int(np.ceil(len(self.filenames) / self.batch_size))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Shuffle file order at the end of each epoch if shuffle=True.\"\"\"\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.filenames)\n",
        "\n",
        "    def normalize_channel(self, image):\n",
        "        \"\"\"\n",
        "        Percentile-based normalization after CLAHE/blur.\n",
        "\n",
        "        Inputs:\n",
        "            image (np.ndarray): grayscale channel.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: normalized channel in [0, 1].\n",
        "        \"\"\"\n",
        "        per99 = np.percentile(image, 99)\n",
        "        per1  = np.percentile(image, 1)\n",
        "        image = np.clip(image, per1, per99)\n",
        "        return (image - per1) / (per99 - per1 + 1e-8)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Build one batch.\n",
        "\n",
        "        Inputs:\n",
        "            index (int): batch index.\n",
        "\n",
        "        Returns:\n",
        "            images (np.ndarray): (B, SIZE, SIZE, 2), float32.\n",
        "            masks  (np.ndarray): (B, SIZE, SIZE, NUM_CLASSES), one-hot.\n",
        "        \"\"\"\n",
        "        batch_filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        images, masks = [], []\n",
        "\n",
        "        for base in batch_filenames:\n",
        "            ch1_path = os.path.join(self.image_dir, f\"{base}_ch1.tif\")\n",
        "            ch2_path = os.path.join(self.image_dir, f\"{base}_ch2.tif\")\n",
        "            mask_path = os.path.join(self.mask_dir,   f\"{base}.tif\")\n",
        "\n",
        "            ch1 = cv2.imread(ch1_path, cv2.IMREAD_GRAYSCALE)\n",
        "            ch2 = cv2.imread(ch2_path, cv2.IMREAD_GRAYSCALE)\n",
        "            msk = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "            if ch1 is None or ch2 is None or msk is None:\n",
        "                continue\n",
        "\n",
        "            ch1 = cv2.resize(ch1, (self.SIZE, self.SIZE), interpolation=cv2.INTER_AREA)\n",
        "            ch2 = cv2.resize(ch2, (self.SIZE, self.SIZE), interpolation=cv2.INTER_AREA)\n",
        "            msk = cv2.resize(msk, (self.SIZE, self.SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            ch1 = self.normalize_channel(self.clahe.apply(cv2.GaussianBlur(ch1, (3,3), 0)))\n",
        "            ch2 = self.normalize_channel(self.clahe.apply(cv2.GaussianBlur(ch2, (3,3), 0)))\n",
        "            img = np.stack([ch1, ch2], axis=-1).astype(np.float32)\n",
        "\n",
        "            msk = msk.copy()\n",
        "            msk[msk == 85]  = 1\n",
        "            msk[msk == 170] = 2\n",
        "            msk[msk == 255] = 3\n",
        "\n",
        "            images.append(img)\n",
        "            masks.append(msk)\n",
        "\n",
        "        images = np.array(images, dtype=np.float32)\n",
        "        masks  = np.array(masks,  dtype=np.uint8)\n",
        "        masks  = to_categorical(masks, num_classes=self.NUM_CLASSES)\n",
        "        return images, masks\n",
        "\n",
        "# =========================\n",
        "# TRAIN/VAL SPLIT\n",
        "# =========================\n",
        "# Purpose:\n",
        "#   Collect base names from MASK_DIR and split into train/val by VAL_SPLIT and RANDOM_STATE.\n",
        "all_filenames = sorted([f.replace(\".tif\", \"\") for f in os.listdir(MASK_DIR) if f.endswith(\".tif\")])\n",
        "train_filenames, val_filenames = train_test_split(\n",
        "    all_filenames, test_size=VAL_SPLIT, random_state=RANDOM_STATE\n",
        ")\n",
        "train_gen = TiffDataGenerator(IMAGE_DIR, MASK_DIR, train_filenames, batch_size=BATCH_SIZE)\n",
        "val_gen   = TiffDataGenerator(IMAGE_DIR, MASK_DIR, val_filenames,   batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# =========================\n",
        "# MODEL + COMPILE + RESUME\n",
        "# =========================\n",
        "# Purpose:\n",
        "#   Build the network, compile with the weighted loss, and optionally resume training\n",
        "#   from the last saved epoch/weights using CSV length and model_last.weights.h5.\n",
        "model = build_unet((SIZE, SIZE, IN_CHANNELS))\n",
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "              loss=loss_fn,\n",
        "              metrics=[\"accuracy\"],\n",
        "              run_eagerly=True)\n",
        "\n",
        "initial_epoch = 0\n",
        "if RESUME_TRAINING:\n",
        "    if os.path.exists(LOG_PATH):\n",
        "        try:\n",
        "            log_df = pd.read_csv(LOG_PATH)\n",
        "            initial_epoch = len(log_df)\n",
        "        except Exception:\n",
        "            initial_epoch = 0\n",
        "    if os.path.exists(WEIGHTS_PATH):\n",
        "        print(\"Loading weights from previous checkpoint...\")\n",
        "        model.load_weights(WEIGHTS_PATH)\n",
        "\n",
        "# =========================\n",
        "# CALLBACKS\n",
        "# =========================\n",
        "class PeriodicSaver(Callback):\n",
        "    \"\"\"\n",
        "    Save weights every N epochs and (optionally) a full model at a fixed cadence.\n",
        "\n",
        "    Init Inputs:\n",
        "        save_every (int): frequency (in epochs) for saving weight snapshots.\n",
        "        save_dir (str): target folder for the snapshots.\n",
        "        prefix (str): filename prefix for weight snapshots.\n",
        "\n",
        "    Behavior:\n",
        "        - Always saves weights every `save_every` epochs as <prefix>_<epoch>.weights.h5\n",
        "        - If SAVE_FULL_MODEL_EVERY_N is set (int), also saves model_epoch_<epoch>.h5 periodically.\n",
        "    \"\"\"\n",
        "    def __init__(self, save_every, save_dir, prefix=\"weights_epoch\"):\n",
        "        super().__init__()\n",
        "        self.save_every = save_every\n",
        "        self.save_dir = save_dir\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.save_every == 0:\n",
        "            w_path = os.path.join(self.save_dir, f\"{self.prefix}_{epoch+1}.weights.h5\")\n",
        "            self.model.save_weights(w_path)\n",
        "            print(f\"\\nSaved weights at epoch {epoch+1} -> {w_path}\")\n",
        "        if SAVE_FULL_MODEL_EVERY_N and (epoch + 1) % SAVE_FULL_MODEL_EVERY_N == 0:\n",
        "            f_path = os.path.join(self.save_dir, f\"model_epoch_{epoch+1}.h5\")\n",
        "            self.model.save(f_path)\n",
        "            print(f\"Saved full model at epoch {epoch+1} -> {f_path}\")\n",
        "\n",
        "checkpoint_last = ModelCheckpoint(\n",
        "    WEIGHTS_PATH,\n",
        "    save_best_only=False,\n",
        "    save_weights_only=True,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        ")\n",
        "checkpoint_best = ModelCheckpoint(\n",
        "    BEST_WEIGHTS_PATH,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        ")\n",
        "\n",
        "class SaveFullBest(Callback):\n",
        "    \"\"\"\n",
        "    Save a full model file when a new best validation loss is reached.\n",
        "\n",
        "    Init Inputs:\n",
        "        best_path (str): output path for the best full model file.\n",
        "\n",
        "    Behavior:\n",
        "        - Active only if SAVE_FULL_MODEL_EVERY_N is set (not None).\n",
        "        - Tracks best val_loss and saves FULL_MODEL_BEST_PATH when improved.\n",
        "    \"\"\"\n",
        "    def __init__(self, best_path):\n",
        "        super().__init__()\n",
        "        self.best = np.inf\n",
        "        self.best_path = best_path\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if not SAVE_FULL_MODEL_EVERY_N:\n",
        "            return\n",
        "        val_loss = logs.get(\"val_loss\", None)\n",
        "        if val_loss is not None and val_loss < self.best:\n",
        "            self.best = val_loss\n",
        "            self.model.save(self.best_path)\n",
        "            print(f\"Saved FULL MODEL (best) -> {self.best_path}\")\n",
        "\n",
        "full_best_cb = SaveFullBest(FULL_MODEL_BEST_PATH) if SAVE_FULL_MODEL_EVERY_N else None\n",
        "csv_logger = CSVLogger(LOG_PATH, append=True)\n",
        "periodic_saver = PeriodicSaver(save_every=SAVE_WEIGHTS_EVERY_N, save_dir=SAVE_DIR)\n",
        "\n",
        "callbacks = [checkpoint_last, checkpoint_best, csv_logger, periodic_saver]\n",
        "if full_best_cb:\n",
        "    callbacks.append(full_best_cb)\n",
        "\n",
        "# =========================\n",
        "# TRAIN\n",
        "# =========================\n",
        "# Purpose:\n",
        "#   Run model.fit with (optional) resume support via initial_epoch and preloaded weights.\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=EPOCHS,\n",
        "    initial_epoch=initial_epoch,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Optional: save a final full model at the end (enabled only if SAVE_FULL_MODEL_EVERY_N is set)\n",
        "if SAVE_FULL_MODEL_EVERY_N:\n",
        "    model.save(FULL_MODEL_LAST_PATH)\n"
      ]
    }
  ]
}