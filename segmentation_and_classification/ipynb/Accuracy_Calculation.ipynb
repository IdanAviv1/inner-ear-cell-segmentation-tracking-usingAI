{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Segmentation Metrics Evaluation (GT TIFF vs. Prediction TIFF)\n",
        "\n",
        "What it does\n",
        "------------\n",
        "- Reads GT frames from a TIFF (odd pages), applies undefined->3, transposes first N if requested,\n",
        "  and tracks fully-black frames.\n",
        "- Reads predicted frames from a multi-page TIFF (output of the prediction script),\n",
        "  mapping {0,85,170,255} or {0,1,2,3} to class indices {0..3}; detects fully-black pages.\n",
        "- Skips any frame that is black on either side (union), aligns shapes (transpose GT if needed),\n",
        "  and computes per-frame and global pixel-wise metrics: IoU / Precision / Recall / F1 for classes {0,1,2,3}.\n",
        "- Exports ONLY a per-frame CSV named 'frame_metrics_from_folders.csv' to OUT_DIR.\n",
        "\n",
        "Requirements\n",
        "------------\n",
        "Python >= 3.9\n",
        "pip install numpy opencv-python tifffile scipy pandas scikit-learn tqdm\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# =========================\n",
        "# USER CONFIG (EDIT HERE)\n",
        "# =========================\n",
        "GT_TIFF_PATH   = \"/content/drive/MyDrive/Final_Project/movies/position4_seg_E17.5_231221.tif\"   # GT TIFF\n",
        "PRED_TIFF_PATH = \"/content/drive/MyDrive/Final_Project/output_predictions/all_predictions.tif\"  # prediction TIFF (multi-page)\n",
        "OUT_DIR        = \"/content/drive/MyDrive/Final_Project/output_predictions\"                      # output folder for the CSV\n",
        "\n",
        "TRANSPOSE_FIRST_N = 1000   # apply cv2.transpose to the first N GT frames (to match your convention)\n",
        "MAX_FRAMES        = None   # None = use all frames; or int to cap GT frames read\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# =========================\n",
        "# Helpers\n",
        "# =========================\n",
        "def is_all_black(u8: np.ndarray) -> bool:\n",
        "    \"\"\"\n",
        "    Check if an image is entirely zeros.\n",
        "\n",
        "    Args:\n",
        "        u8 (np.ndarray): grayscale image.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if non-empty and all pixels are 0.\n",
        "    \"\"\"\n",
        "    return u8 is not None and u8.size > 0 and (u8.max() == 0)\n",
        "\n",
        "\n",
        "def png_to_classes(u8: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Map a grayscale mask to class indices {0,1,2,3}.\n",
        "    Accepts either {0,1,2,3} or {0,85,170,255}. Uses robust thresholds.\n",
        "\n",
        "    Args:\n",
        "        u8 (np.ndarray): grayscale prediction.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: class-index map in {0..3}, dtype uint8.\n",
        "    \"\"\"\n",
        "    uniq = np.unique(u8)\n",
        "    if set(uniq.tolist()).issubset({0, 1, 2, 3}):\n",
        "        return u8.astype(np.uint8)\n",
        "    cls = np.zeros_like(u8, dtype=np.uint8)\n",
        "    cls[u8 >= 213] = 3\n",
        "    cls[(u8 >= 128) & (u8 < 213)] = 2\n",
        "    cls[(u8 >=  42) & (u8 < 128)] = 1\n",
        "    return cls\n",
        "\n",
        "# =========================\n",
        "# GT reader (unchanged logic)\n",
        "# =========================\n",
        "def read_gt_frames_from_tif(tiff_path: str, transpose_first_n: int = 1000, max_frames=None):\n",
        "    \"\"\"\n",
        "    Read GT from a TIFF (odd pages), apply undefined->3, transpose first N, detect black frames.\n",
        "\n",
        "    Args:\n",
        "        tiff_path (str): path to GT TIFF.\n",
        "        transpose_first_n (int): apply cv2.transpose to the first N frames.\n",
        "        max_frames (int|None): limit the number of frames read.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            - gt_full (list[Optional[np.ndarray]]): per-frame class maps (0..3) or None if black.\n",
        "            - black_idx_gt (set[int]): indices of fully black frames.\n",
        "    \"\"\"\n",
        "    import tifffile as tiff\n",
        "    from scipy.ndimage import binary_dilation\n",
        "\n",
        "    def separate_undefined_regions(frame: np.ndarray) -> np.ndarray:\n",
        "        frame = frame.copy()\n",
        "        zero_mask = (frame == 0)\n",
        "        cell_mask = (frame == 1) | (frame == 2)\n",
        "        dilated_cells = binary_dilation(cell_mask)\n",
        "        undefined_mask = zero_mask & (~dilated_cells)\n",
        "        frame[undefined_mask] = 3\n",
        "        return frame\n",
        "\n",
        "    gt_full = []       # length N: np.ndarray (classes 0..3) or None if black\n",
        "    black_idx_gt = []  # indices of black frames in the TIFF\n",
        "\n",
        "    with tiff.TiffFile(tiff_path) as tif:\n",
        "        all_indices = list(range(1, len(tif.pages), 2))  # masks at odd pages\n",
        "        if max_frames is not None:\n",
        "            all_indices = all_indices[:max_frames]\n",
        "\n",
        "        frame_counter = 0\n",
        "        for i in tqdm(all_indices, desc=\"Reading GT from TIFF\"):\n",
        "            frame = tif.pages[i].asarray()\n",
        "            idx = frame_counter\n",
        "            if np.all(frame == 0):\n",
        "                gt_full.append(None)\n",
        "                black_idx_gt.append(idx)\n",
        "            else:\n",
        "                if idx < transpose_first_n:\n",
        "                    frame = cv2.transpose(frame)\n",
        "                cleaned = separate_undefined_regions(frame).astype(np.uint8)\n",
        "                gt_full.append(cleaned)\n",
        "            frame_counter += 1\n",
        "\n",
        "    return gt_full, set(black_idx_gt)\n",
        "\n",
        "# =========================\n",
        "# Prediction reader (TIFF)\n",
        "# =========================\n",
        "def read_pred_from_tif(pred_tif_path: str):\n",
        "    \"\"\"\n",
        "    Read predictions from a multi-page TIFF.\n",
        "\n",
        "    Args:\n",
        "        pred_tif_path (str): path to prediction TIFF produced by the prediction script.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            - pred_map (dict[int, Optional[np.ndarray]]): index -> class map (0..3) or None if black.\n",
        "            - black_idx_pred (set[int]): indices of black prediction frames.\n",
        "    \"\"\"\n",
        "    import tifffile as tiff\n",
        "\n",
        "    pred_map = {}\n",
        "    black_idx_pred = set()\n",
        "\n",
        "    with tiff.TiffFile(pred_tif_path) as tif:\n",
        "        pages = len(tif.pages)\n",
        "        if pages == 0:\n",
        "            raise RuntimeError(f\"No images found in {pred_tif_path}\")\n",
        "\n",
        "        for idx in tqdm(range(pages), desc=\"Reading predictions from TIFF\"):\n",
        "            u8 = tif.pages[idx].asarray()\n",
        "            if u8 is None:\n",
        "                continue\n",
        "            if is_all_black(u8):\n",
        "                pred_map[idx] = None\n",
        "                black_idx_pred.add(idx)\n",
        "            else:\n",
        "                pred_map[idx] = png_to_classes(u8.astype(np.uint8))\n",
        "\n",
        "    return pred_map, black_idx_pred\n",
        "\n",
        "# =========================\n",
        "# Evaluation (logic preserved)\n",
        "# =========================\n",
        "def evaluate_tif_vs_folder(gt_tif=GT_TIFF_PATH, pred_dir=PRED_TIFF_PATH, out_dir=OUT_DIR,\n",
        "                           transpose_first_n=TRANSPOSE_FIRST_N, max_frames=MAX_FRAMES):\n",
        "    \"\"\"\n",
        "    Compute per-frame and global pixel-wise metrics between GT TIFF and prediction TIFF.\n",
        "\n",
        "    Prints:\n",
        "        - Per-class frame-wise means.\n",
        "        - Global pixel-wise metrics and weighted aggregates.\n",
        "        - Final accuracy aggregates.\n",
        "\n",
        "    Side effects:\n",
        "        - Saves a single CSV 'frame_metrics_from_folders.csv' with one row per frame,\n",
        "          starting with the four aggregates you requested, followed by per-class metrics.\n",
        "\n",
        "    Args:\n",
        "        gt_tif (str): path to GT TIFF.\n",
        "        pred_dir (str): path to predictions TIFF (multi-page).\n",
        "        out_dir (str): output directory for the CSV.\n",
        "        transpose_first_n (int): transpose first N GT frames.\n",
        "        max_frames (int|None): cap frames read from GT.\n",
        "    \"\"\"\n",
        "    gt_full, black_gt = read_gt_frames_from_tif(gt_tif, transpose_first_n, max_frames)\n",
        "    pred_map, black_pred = read_pred_from_tif(pred_dir)\n",
        "\n",
        "    N = len(gt_full)\n",
        "    print(f\"\\nTotal frames (by TIFF): {N}\")\n",
        "    print(f\"Black in GT (TIFF): {len(black_gt)} | Black in predictions (folder): {len(black_pred)}\")\n",
        "\n",
        "    # Ignore any frame black on either side (union)\n",
        "    skip = black_gt | black_pred\n",
        "\n",
        "    # Accumulators\n",
        "    results = []\n",
        "    ious_list, precisions_list, recalls_list, f1s_list = [], [], [], []\n",
        "    gt_all_list, pred_all_list = [], []\n",
        "\n",
        "    # Valid indices: present in pred_map, not skipped, GT not None\n",
        "    valid_indices = [i for i in range(N) if (i in pred_map) and (i not in skip) and (gt_full[i] is not None)]\n",
        "    if not valid_indices:\n",
        "        raise RuntimeError(\"No valid pairs to compare (after skipping black frames and unmatched indices).\")\n",
        "\n",
        "    # Iterate all frames (kept as in original loop structure)\n",
        "    for i in tqdm(range(N), desc=\"Evaluating\"):\n",
        "        if i not in pred_map:\n",
        "            continue\n",
        "        if i in skip:\n",
        "            continue\n",
        "        gt = gt_full[i]\n",
        "        pred = pred_map[i]\n",
        "        if gt is None or pred is None:\n",
        "            continue\n",
        "\n",
        "        # Dimension alignment â€” transpose GT if exactly reversed\n",
        "        if gt.shape != pred.shape:\n",
        "            if gt.shape[::-1] == pred.shape:\n",
        "                gt = cv2.transpose(gt)\n",
        "            else:\n",
        "                raise ValueError(f\"Shape mismatch at frame {i}: GT={gt.shape}, Pred={pred.shape}\")\n",
        "\n",
        "        gt_flat   = gt.flatten()\n",
        "        pred_flat = pred.flatten()\n",
        "\n",
        "        cm = confusion_matrix(gt_flat, pred_flat, labels=[0, 1, 2, 3])\n",
        "        inter = np.diag(cm)\n",
        "        union = np.sum(cm, axis=1) + np.sum(cm, axis=0) - inter\n",
        "        class_iou = inter / np.maximum(union, 1)\n",
        "\n",
        "        precision = precision_score(gt_flat, pred_flat, average=None, labels=[0, 1, 2, 3], zero_division=0)\n",
        "        recall    = recall_score(gt_flat,  pred_flat, average=None, labels=[0, 1, 2, 3], zero_division=0)\n",
        "        f1        = f1_score(gt_flat,     pred_flat, average=None, labels=[0, 1, 2, 3], zero_division=0)\n",
        "\n",
        "        # ---- Per-frame aggregates (FIRST in the row) ----\n",
        "        support_frame = np.bincount(gt_flat, minlength=4)\n",
        "        weighted_iou_frame = float(np.average(class_iou, weights=support_frame)) if support_frame.sum() > 0 else float(\"nan\")\n",
        "        weighted_f1_frame  = float(np.average(f1,        weights=support_frame)) if support_frame.sum() > 0 else float(\"nan\")\n",
        "        cell_macro_iou_fr  = float(np.mean(class_iou[[1, 2]]))\n",
        "        cell_macro_f1_fr   = float(np.mean(f1[[1, 2]]))\n",
        "\n",
        "        results.append({\n",
        "            \"frame\": i,\n",
        "            \"Cell_Macro_IoU_1_2\": cell_macro_iou_fr,\n",
        "            \"Cell_Macro_F1_1_2\":  cell_macro_f1_fr,\n",
        "            \"Weighted_IoU_0_3\":   weighted_iou_frame,\n",
        "            \"Weighted_F1_0_3\":    weighted_f1_frame,\n",
        "            **{f\"IoU_{j}\": float(class_iou[j]) for j in range(4)},\n",
        "            **{f\"Precision_{j}\": float(precision[j]) for j in range(4)},\n",
        "            **{f\"Recall_{j}\": float(recall[j]) for j in range(4)},\n",
        "            **{f\"F1_{j}\": float(f1[j]) for j in range(4)},\n",
        "        })\n",
        "\n",
        "        ious_list.append(class_iou)\n",
        "        precisions_list.append(precision)\n",
        "        recalls_list.append(recall)\n",
        "        f1s_list.append(f1)\n",
        "\n",
        "        gt_all_list.append(gt_flat)\n",
        "        pred_all_list.append(pred_flat)\n",
        "\n",
        "    if not results:\n",
        "        raise RuntimeError(\"No results computed. Check file naming/indexing and black-frame handling.\")\n",
        "\n",
        "    # Frame-wise means (prints kept)\n",
        "    ious_arr       = np.array(ious_list)\n",
        "    precisions_arr = np.array(precisions_list)\n",
        "    recalls_arr    = np.array(recalls_list)\n",
        "    f1s_arr        = np.array(f1s_list)\n",
        "\n",
        "    print(\"\\nðŸ“Š Averaged Metrics (frame-wise means):\")\n",
        "    for j in range(4):\n",
        "        print(f\"Class {j} - IoU: {np.nanmean(ious_arr[:, j]):.4f}, \"\n",
        "              f\"Precision: {np.nanmean(precisions_arr[:, j]):.4f}, \"\n",
        "              f\"Recall: {np.nanmean(recalls_arr[:, j]):.4f}, \"\n",
        "              f\"F1: {np.nanmean(f1s_arr[:, j]):.4f}\")\n",
        "\n",
        "    # Global (pixel-wise over all used frames) â€” prints kept\n",
        "    gt_all   = np.concatenate(gt_all_list, axis=0)\n",
        "    pred_all = np.concatenate(pred_all_list, axis=0)\n",
        "\n",
        "    cm_global = confusion_matrix(gt_all, pred_all, labels=[0, 1, 2, 3])\n",
        "    support   = np.bincount(gt_all, minlength=4)\n",
        "    inter_g   = np.diag(cm_global)\n",
        "    union_g   = np.sum(cm_global, axis=1) + np.sum(cm_global, axis=0) - inter_g\n",
        "    class_iou_g = inter_g / np.maximum(union_g, 1)\n",
        "\n",
        "    precision_g = precision_score(gt_all, pred_all, average=None, labels=[0, 1, 2, 3], zero_division=0)\n",
        "    recall_g    = recall_score(gt_all,  pred_all, average=None, labels=[0, 1, 2, 3], zero_division=0)\n",
        "    f1_g        = f1_score(gt_all,     pred_all, average=None, labels=[0, 1, 2, 3], zero_division=0)\n",
        "\n",
        "    print(\"\\nðŸ“Š Global Pixel-wise Metrics:\")\n",
        "    for j in range(4):\n",
        "        print(f\"Class {j} - IoU: {class_iou_g[j]:.4f} (support: {int(support[j])})\")\n",
        "\n",
        "    weighted_mean_iou = np.average(class_iou_g, weights=support)\n",
        "    weighted_precision = np.average(precision_g, weights=support)\n",
        "    weighted_recall = np.average(recall_g, weights=support)\n",
        "    weighted_f1 = np.average(f1_g, weights=support)\n",
        "\n",
        "    print(f\"\\nWeighted Mean IoU:   {weighted_mean_iou:.4f}\")\n",
        "    print(f\"Weighted Precision:  {weighted_precision:.4f}\")\n",
        "    print(f\"Weighted Recall:     {weighted_recall:.4f}\")\n",
        "    print(f\"Weighted F1 Score:   {weighted_f1:.4f}\")\n",
        "\n",
        "    # Final accuracy aggregates (prints kept)\n",
        "    cell_macro_iou = float(np.mean(class_iou_g[[1, 2]]))\n",
        "    cell_macro_f1  = float(np.mean(f1_g[[1, 2]]))\n",
        "\n",
        "    print(\"\\n=== Final Accuracy (as reported in the project) ===\")\n",
        "    print(f\"Cell Macro IoU (classes 1â€“2): {cell_macro_iou:.4f}\")\n",
        "    print(f\"Cell Macro F1  (classes 1â€“2): {cell_macro_f1:.4f}\")\n",
        "    print(f\"Weighted IoU (0â€“3):           {weighted_mean_iou:.4f}\")\n",
        "    print(f\"Weighted F1  (0â€“3):           {weighted_f1:.4f}\")\n",
        "\n",
        "    # Save ONLY the per-frame CSV (first columns: the four aggregates you requested)\n",
        "    df = pd.DataFrame(results).sort_values(\"frame\")\n",
        "    csv_path = os.path.join(out_dir, \"frame_metrics_from_folders.csv\")\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\nâœ… Saved per-frame CSV to: {csv_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    evaluate_tif_vs_folder()\n"
      ],
      "metadata": {
        "id": "4K-LohIoicov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa65fb4d-95d6-493f-ffb3-3178bf5d17a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading GT from TIFF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 273/273 [00:26<00:00, 10.16it/s]\n",
            "Reading predictions from TIFF: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 273/273 [00:26<00:00, 10.43it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total frames (by TIFF): 273\n",
            "Black in GT (TIFF): 52 | Black in predictions (folder): 52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 273/273 [06:40<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š Averaged Metrics (frame-wise means):\n",
            "Class 0 - IoU: 0.6675, Precision: 0.7225, Recall: 0.8972, F1: 0.8003\n",
            "Class 1 - IoU: 0.8793, Precision: 0.9429, Recall: 0.9282, F1: 0.9353\n",
            "Class 2 - IoU: 0.8913, Precision: 0.9183, Recall: 0.9678, F1: 0.9414\n",
            "Class 3 - IoU: 0.3213, Precision: 0.7708, Recall: 0.3372, F1: 0.4373\n",
            "\n",
            "ðŸ“Š Global Pixel-wise Metrics:\n",
            "Class 0 - IoU: 0.6672 (support: 33884291)\n",
            "Class 1 - IoU: 0.8793 (support: 180931758)\n",
            "Class 2 - IoU: 0.8901 (support: 544693430)\n",
            "Class 3 - IoU: 0.4015 (support: 65400425)\n",
            "\n",
            "Weighted Mean IoU:   0.8398\n",
            "Weighted Precision:  0.9144\n",
            "Weighted Recall:     0.9127\n",
            "Weighted F1 Score:   0.9055\n",
            "\n",
            "=== Final Accuracy (as reported in the project) ===\n",
            "Cell Macro IoU (classes 1â€“2): 0.8847\n",
            "Cell Macro F1  (classes 1â€“2): 0.9388\n",
            "Weighted IoU (0â€“3):           0.8398\n",
            "Weighted F1  (0â€“3):           0.9055\n",
            "\n",
            "âœ… Saved per-frame CSV to: /content/drive/MyDrive/Final_Project/output_predictions/frame_metrics_from_folders.csv\n"
          ]
        }
      ]
    }
  ]
}